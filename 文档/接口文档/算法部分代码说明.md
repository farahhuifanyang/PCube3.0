<!--
 * @Author: 李泽坤
 * @Date: 2021-05-27 20:51:21
 * @LastEditTime: 2021-06-09 11:30:11
 * @LastEditors: Please set LastEditors
 * @Description: In User Settings Edit
 * @FilePath: /PCube3.0/文档/接口文档/算法部分代码说明.md
-->
# 人立方3.0 算法部分代码说明
本markdown文档说明了人立方3.0项目中用python语言撰写的核心算法部分的说明，它将主要包括以下几个部分：
- 文件结构：本部分的文件结构及各个文件的主要功能
- 爬虫部分：对爬虫部分代码的详细说明和使用方法
- 预处理部分：对预处理部分代码的详细说明和使用方法
- 核心算法部分：对核心NLP算法的大致描述
    - 实体识别
    - 实体链接
    - 开放式关系抽取
    - 文章事件聚类
    - 事件摘要提取
    - 人格分析
- 基础数据处理：对基础数据进行处理的代码的详细说明和使用方法
- 定时数据处理：对每天产生的数据进行处理的代码的详细说明和使用方法
- 数据库读写：将处理完成的数据存入数据库

## 1. 文件结构
【需要补充】
```
crawl/    爬虫代码
├── crawl_chinatimes.py    爬取《中时电子报》文章的代码
├── crawl_facebook.py    爬取facebook博文的代码
├── crawl_ltn    使用scrapy爬取《自由时报》的代码
│   ├── crawl_ltn
│   │   ├── __init__.py
│   │   ├── items.py
│   │   ├── middlewares.py
│   │   ├── pipelines.py
│   │   ├── settings.py
│   │   └── spiders
│   │       ├── __init__.py
│   │       └── url.py
│   ├── __init__.py
│   └── scrapy.cfg
└── crawl_ltn.py

preprocess/    预处理代码
├── ann2train.py    将使用inception标注的文章转化为训练使用的数据
├── convertNER2EL.py    将ann2train处理得到的实体数据转化成实体链接训练用数据的代码
├── forAnnotationLTN.py    将《自由时报》的文章转换成inception可以识别的格式
├── forAnnotation.py    将《中时电子报》的文章转换成inception可以识别的格式
├── forAnnotation UDN.py    将《联合报》的文章转换成inception可以识别的格式
├── forCluster.py    文本聚类前的预处理步骤
├── forPersonality.py    为人格分析处理社交网站数据的代码
├── forRealTime.py    在定时处理时使用的文章预处理代码
├── __init__.py
├── LTPtagger.py    封装了哈工大LTP的自动标注功能
└── SparqWrapper.py    用于获取两个实体在Wikidata中的关系的代码

coreNLP/    核心NLP代码
├── Algorithm.py    所有算法接口的基类
├── CLUSTER    文章聚类
│   ├── config.py
│   ├── dataParser.py
│   └── forCluster.py
├── EL    实体链接
│   ├── config.py    实体链接部分的配置，不允许甲方或用户修改
│   ├── dataParser.py    用来处理实体链接数据
│   ├── main.py    实体链接的主要算法部分，从多个歧义项中找出唯一的正确项
│   ├── model.py    定义了实体链接的模型
│   └── WikiLinker.py    实体链接算法特有的部分，用以从数据库和wikidata寻找候选实体
├── NER    实体识别
│   ├── config.py    实体识别部分的配置，不允许甲方或用户修改
│   ├── dataParser.py    处理实体识别的数据
│   ├── main.py    实体识别的主要算法部分，从来自多篇文章的多个句子中抽取实体
│   └── model.py    定义了实体识别的模型
├── PER    人格分析
│   ├── config.py
│   ├── dataParser.py
│   ├── main.py
│   └── model.py
├── processOnBaseNewsData.py    实现了用核心NLP算法依次处理文章数据的算法流程
├── transferOldData.py    实现了从旧数据库里导出数据到新数据库的流程
└── SUM    摘要
    ├── config.py
    ├── DataParser.py
    ├── main.py
    └── model.py

DAO/    读写数据库的接口
├── Elastic    读写ES特定表的接口目录
│   └── EntityFile.py    读写ES中实体详细数据的
├── ElasticFile.py    读写ES文件的接口基类
├── Hbase    读写HBase的接口
│   ├── EntityAlias.py    实体别名表
│   └── Entity.py    实体表，弃用
├── HbaseTable.py    读写HBase的接口基类
├── Neo4jEdge.py    读写Neo4j的实体关系
└── Neo4jNode.py    读写Neo4j的实体节点

static/    代码运行所需的少量静态文件
├── crawl
│   ├── chinatimes_topics.json    《中时电子报》爬虫所需的目标话题列表
│   └── ltn_topics.json    《自由时报》爬虫所需的目标话题列表
├── preproces
│   ├── AnnFileMap.json    《中时》用于标注的文件名与原文件名的对应
│   ├── CannotLink.json    为ann2train.py使用的标注中无法消歧但实际上wikidata中存在的实体
│   ├── LTNAnnFileMap.json    用于标注的文件名与原文件名的对应
│   ├── stop_word.txt    文本聚类使用的停用词表
│   └── UDNAnnFileMap.json    用于标注的文件名与原文件名的对应
└── processOnBaseData
```


## 2. 爬虫部分
### 2.1 爬取《中时电子报》
相关代码均被收录在 crawl/crawl_chinatimes.py中，它大致分为两个部分流程
#### 2.1.1 爬取《中时电子报》的历史数据
执行的入口函数为 history_news 其过程大致如下：
1. 爬取近期的文章URL，即《中时电子报》10页索引页面的全部文章的URL
2. 根据第1步得到的URL列表进一步分析每个文章页面的信息，得到相应的文章
3. 重复第2步直至取得URL列表中所有的文章，暂存到/home/disk2/nuclear/news_data/PCube/CT
4. 根据第三步完成后得到的文章集合，对所有文章包含的关键词进行统计，得到关键词表
5. 用关键词表在《中时电子报》网站上进行搜索，得到包含该关键词的2019年1月及之后的所有文章列表
6. 重复执行第2步，直到所有列表中所有的文章都爬取完成，暂存到/home/disk2/nuclear/news_data/PCube/CT
#### 2.1.2 定时爬取《中时电子报》
执行的入口函数为 main 其过程大致如下：
1. 爬取本日的文章URL，即《中时电子报》索引页面中当日全部文章的URL
2. 根据第1步得到的URL列表进一步分析每个文章页面的信息，得到相应的文章
3. 重复第2步直至取得URL列表中所有的文章，暂存到/home/disk2/nuclear/news_data/PCube/CT

### 2.2 爬取《联合报》
其过程与爬取中时电子报基本一致

### 2.3 爬取《自由时报》
爬取自由时报时采用了scrapy框架【需要补充】


## 3. 预处理部分
### 3.1 从文章预处理得到标注用数据
相关代码均被收录在 preprocess/forAnnotation.py 中  
执行的入口函数为 randomSample，主要分为以下几个步骤：
1. 在已有的文章中随机采样若干篇用于标注
2. 对每篇文章使用LTP进行处理，标注实体用于标注参考

### 3.2 从标注数据处理得到训练用数据
相关代码被收录在 preprocess/ann2train.py 和 preprocess/convertNER2EL.py 中  
ann2train.parseNERData 为NER数据处理的入口  
ann2train.parseOREQAData 为ORE数据处理的入口  
convertNER2EL.NER2EL 及 addNegativeSample需要接续运行，为从NER数据处理获得EL数据的入口  
具体的流程比较复杂，请参阅代码本身的注释

### 3.3 从文章预处理得到文章聚类用数据
【需要补充】

### 3.4 从社交网站预处理得到人格分析用数据
【需要补充】

### 3.5 从文章预处理得到定时处理用数据
【需要补充】


## 4. 核心算法部分
该部分算法的主题部分均以算法类定义，coreNLP/Algorithm.py是它们的基类
### 4.1 实体识别
代码收录在 coreNLP/NER 目录下  
本部分采用BERT+CRF的网络模型，融合词类（POS）信息，以序列标注的形式进行实体抽取，一次接口调用大致包括以下步骤
1. 输入处理：算法每次输入不限定个数的文章，每个文章包含多条句子，算法首先将所有的句子拆解出来，将句子组装成batch，并记录句子与原文的对应
2. 嵌入层：将输入的句子和POS标签转化为数字并Tensor化，用BERT对句子进行编码，用随机产生的嵌入对POS编码，链接二者作为句子嵌入表示
3. CRF层：用CRF的维特比方法对序列进行解码，得到BIO标签序列
4. 后处理：CRF的直接输出存在部分单字实体的假阳性问题，即并非是实体的字被分为了实体。考虑到新闻文本的规范性，若模型预测出单字实体时，该篇文章之前的实体没有以此开头的实体，则认为实体是非法的。最后将句子组装回文章，并标注每个句子里实体的位置和内容
  
单纯使用神经网络模型输出的实体评测结果如下，评测数据集为标注的500篇真实文章：
``` 
Precision:  0.8492, Recall:  0.9228, F1:  0.8845
```

### 4.2 实体链接
代码收录在 coreNLP/EL 目录下  
本部分采用BERT+Co-attention网络模型，一次接口调用大致包括以下步骤：
1. 输入处理：算法每次输入一个待消歧实体的原句，form及其若干个候选实体的正式名称，简介
2. 嵌入层：将原句和简介转化为数字并Tensor化，用BERT对句子进行编码
3. Co-attention层：以原句为Q对简介计算Attention，反过来再以简介为Q对原句计算Attention，再将二者链接起来，调整维度+tanh激活为表示置信度的标量
4. 后处理：找出置信度最高的候选实体，若置信度大于阈值，则判定为链接成功

神经网络模型输出的链接评测结果如下，评测数据集为标注的500篇真实文章：
```
Precision:  1.0000, Recall:  0.9516, F1:  0.9752
```

### 4.3 开放式关系抽取
【需要补充】

### 4.4 文章事件聚类
【需要补充】

### 4.5 事件摘要提取
【需要补充】

### 4.6 人格分析
【需要补充】


## 5. 基础数据处理
### 5.1 基础新闻数据处理
代码收录在 coreNLP/processOnBaseNewsData.py 中包括以下处理流程
#### 5.1.1 实体识别流程
入口函数为 NERonBaseData  
解析目前已有的所有数据，以每100篇为单位，进行实体识别操作，将结果暂存到临时位置中  
该步骤的处理时间应在36小时之内，若时间过长请注意是否没能使用GPU并行加速
#### 5.1.2 实体链接流程
入口函数为 ELOnBaseData
解析实体识别的结果，将所有的实体form进行链接，其主要过程如下：
1. 对于每个form，在Hbase别名表中搜索，如果匹配到唯一结果则直接链接完成
2. 若别名表返回多个结果，调用EL接口选取置信度最高的
3. 若别名表无返回，使用form在wikidata中查询，若得到唯一的实体则直接链接完成
4. 若得到消歧页面，调用EL接口选置信度最高且置信度达标的
5. 若wikidata无返回或置信度无达标，则宣布消歧失败
6. 若实体链接由wikidata完成，需将链接到的实体和别名存入别名表

受制于网络请求的速度，该部分的运行时间极长，可能会超过1个月，有条件的情况下请分别在多个机器上进行

### 5.2 基础社交网站数据处理
【需要补充】

## 6. 定时数据处理
【需要补充】

## 7. 数据库读写接口
### 7.1 Neo4j读写接口
#### 7.1.1 Neo4jNode
相关的接口保存在 DAO/Neo4jNode.py 中  
主要实现功能有：
- 根据id查找节点：给出实体的id精确搜索，返回唯一的结果或None，返回的使用方式与dict类似
- 根据属性值查找节点：给出用来筛选的键和值，以dict传入，返回结果list或None
- 搜索所有的实体数据：以list返回数据库里所有的PCube实体
- 写入节点：给出节点的名称和类型保存节点，其他属性可选
- 更新节点：给出节点id和需要更新的键值，修改neo4j中的节点信息
- 删除所有节点：删除所有的Neo4j节点，极度危险，如无必要不要调用
#### 7.1.2 Neo4jEdge
相关的接口保存在 DAO/Neo4jEdge.py 中
主要实现功能有：
- 根据id查找关系：给出至少一个实体的id，搜索该实体或两个实体间的关系
- 写入关系：给出头尾节点的eid和关系名称，保存关系
### 7.2 HBase读写接口
不同的HBase读写接口的功能基本一致，主要实现功能有：
- 根据id查找数据：给出数据的rowkey进行精确搜索，返回唯一的结果dict或None
- 根据属性值查找节点：给出用来筛选的键和值，以dict传入，返回结果list\[dict\]或None
- 写入一行数据：给出数据的rowkey和dict数据，保存到HBase中
### 7.3 ES读写接口
不同的ES读写接口的功能基本一致，主要实现功能有：
- 根据id查找数据：给出数据的id进行精确搜索，对于特定的表已经封装了特定的index
- 根据属性值查找节点：给出用来筛选的键和值，以dict传入，返回结果list\[dict\]或None
- 写入一行数据：给出数据的id和dict数据，保存到ES中
